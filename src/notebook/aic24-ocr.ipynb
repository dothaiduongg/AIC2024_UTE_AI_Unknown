{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q translate\n!pip install -q underthesea==1.3.5a3\n!pip install -q underthesea[deep]\n!pip install -q pyvi\n!pip install -q langdetect\n!pip install -q googletrans==3.1.0a0\n!pip install -q peft\n!pip install bitsandbytes\n!pip install transformers\n!pip install flash-attn\n!pip install -U sentence-transformers\n!pip install xformers\n!pip install einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T13:27:58.629304Z","iopub.execute_input":"2024-08-12T13:27:58.629681Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/JaidedAI/EasyOCR.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DPRContextEncoder, AutoProcessor, DPRContextEncoderTokenizer, BlipModel,TrOCRProcessor, VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, BitsAndBytesConfig, BlipForConditionalGeneration\n#from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom sentence_transformers import SentenceTransformer\n\n\n# tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base', use_fast=False)\n# embedding_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n\nembedding_model= SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport clip\nfrom PIL import Image\nimport faiss\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nimport math\nimport googletrans\nimport translate\nimport glob\nimport underthesea\nimport sys\nimport time\nfrom tqdm import tqdm\nfrom pyvi import ViUtils, ViTokenizer\nfrom difflib import SequenceMatcher\nfrom langdetect import detect\nfrom pathlib import Path\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = Path(os.getcwd()).resolve()\n\n# Add ROOT to sys.path\nsys.path.append(str(ROOT))\n\n# Determine the working directory\nif len(ROOT.parents) > 1:\n    WORK_DIR = ROOT.parents[0]\nelse:\n    WORK_DIR = ROOT  # Fallback to ROOT if it doesn't have enough parents\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\npaths = f\"{WORK_DIR}/data/keyframes\"\ndes_path =  f\"{WORK_DIR}/working//dicts/npy_ocr\"\nos.makedirs(des_path, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Text_Preprocessing():\n    def __init__(self, stopwords_path=f\"{WORK_DIR}/data/dicts/vietnamese-stopwords-dash.txt\"):\n        with open(stopwords_path, 'r', encoding='utf-8') as f:  # Open in text mode for easier string handling\n            self.stop_words = [line.strip() for line in f.readlines()]\n\n    def find_substring(self, string1, string2):\n        match = SequenceMatcher(None, string1, string2, autojunk=False).find_longest_match(0, len(string1), 0, len(string2))\n        return string1[match.a:match.a + match.size].strip()\n\n    def remove_stopwords(self, text):\n        text = ViTokenizer.tokenize(text)\n        filtered_words = [w for w in text.split() if w not in self.stop_words]\n        return \" \".join(filtered_words)\n\n    def lowercasing(self, text):\n        return text.lower()\n\n    def uppercasing(self, text):\n        return text.upper()\n\n    def add_accents(self, text):\n        return ViUtils.add_accents(text)\n\n    def remove_accents(self, text):\n        return ViUtils.remove_accents(text)\n\n    def sentence_segment(self, text):\n        return underthesea.sent_tokenize(text)\n\n    def text_norm(self, text):\n        return underthesea.text_normalize(text)\n\n    def text_classify(self, text):\n        return underthesea.classify(text)\n\n    def sentiment_analysis(self, text):\n        return underthesea.sentiment(text)\n\n    def __call__(self, text):\n        # Apply preprocessing steps\n        text = self.lowercasing(text)\n        text = self.remove_stopwords(text)\n        # Uncomment and adjust as needed\n        # text = self.remove_accents(text)\n        # text = self.add_accents(text)\n        text = self.text_norm(text)\n        return text  # Return the processed text\n\nclass Translation():\n    def __init__(self, from_lang='vi', to_lang='en', mode='google'):\n        # The class Translation is a wrapper for the two translation libraries, googletrans and translate.\n        self.__mode = mode\n        self.__from_lang = from_lang\n        self.__to_lang = to_lang\n        self.text_processing = Text_Preprocessing()\n        if mode in 'googletrans':\n            self.translator = googletrans.Translator()\n        elif mode in 'translate':\n            self.translator = translate.Translator(from_lang=from_lang,to_lang=to_lang)\n\n    def preprocessing(self, text):\n\n        return self.text_processing(text) #text.lower()\n\n    def __call__(self, text):\n\n        text = self.preprocessing(text)\n        return self.translator.translate(text) if self.__mode in 'translate' \\\n                else self.translator.translate(text, dest=self.__to_lang).text\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Function to perform OCR on an image and return text\ndef ocr_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_text\n\ndef easyocr_image(image_path):\n    texts = []\n    result = reader.readtext(image_path)\n\n    for i in result:\n        if i[2] >0.3:\n          texts.append(i[1])\n\n    merged_text = \"\\n\".join(texts)\n    merged_text = Translation(merged_text)\n    return merged_text\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor keyframe in tqdm(os.listdir(paths)):\n  path_keyframe = os.path.join(paths,keyframe)\n  video_paths = sorted(glob.glob(f\"{path_keyframe}/*/\"))\n  video_paths = ['/'.join(i.split('/')[:-1]) for i in video_paths]\n\n  start_time = time.time()\n  for vd_path in video_paths:\n\n    re_feats = []\n    keyframe_paths = glob.glob(f'{vd_path}/*.jpg')\n    keyframe_paths = sorted(keyframe_paths, key=lambda x : x.split('/')[-1].replace('.jpg',''))\n\n    for keyframe_path in tqdm(keyframe_paths):\n\n\n      #text = ocr_image(keyframe_path)\n\n      #//////////////////////////////////\n      text = easyocr_image(keyframe_path)\n      #//////////////////////////////////\n      if detect(text) == 'vi' :\n        text = Translation(text)\n\n      # Convert text to embedding vector\n      #embedding = embedding_model(**tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True,max_length=512, add_special_tokens = True)).pooler_output.detach().numpy()\n      embeddings = embedding_model.encode(text)\n      # Append embedding to re_feats list\n      re_feats.append(embeddings)\n\n    name_npy = vd_path.split('/')[-1]\n\n    # Construct output file path\n    outfile = os.path.join(des_path, f'{name_npy}.npy')\n\n    # Ensure the directory exists before saving\n    os.makedirs(des_path, exist_ok=True)\n    np.save(outfile, re_feats)\n\n    print(f\"Processed {vd_path} in {time.time() - start_time} seconds\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_shape = 384\n\n\ndef write_bin_file_ocr(bin_path: str, npy_path: str, method='cosine'):\n    if method in 'L2':\n      index = faiss.IndexFlatL2(feature_shape)\n    elif method in 'cosine':\n      index = faiss.IndexFlatIP(feature_shape)\n    else:\n      assert f\"{method} not supported\"\n    npy_files = glob.glob(os.path.join(npy_path, \"*.npy\"))\n    npy_files_sorted = sorted(npy_files)\n\n    for npy_file in npy_files_sorted:\n        feats = np.load(npy_file)\n        print(f\"Loaded {npy_file}, shape: {feats.shape}\")\n\n\n        # Convert to float32 and reshape to match feature_shape\n        feats = feats.astype(np.float32)\n        feats = feats.reshape(-1, feats.shape[-1])\n\n        # Resize or trim feats_normalized to match feature_shape if necessary\n        if feats.shape[1] != feature_shape:\n            feats = feats[:, :feature_shape]\n\n        assert feats.shape[1] == feature_shape, \\\n            f\"Query features dimension {feats.shape[1]} do not match index dimension {feature_shape}\"\n\n        # Add to Faiss index\n        index.add(feats)\n\n    # Write the Faiss index to disk\n    faiss.write_index(index, os.path.join(bin_path, f\"faiss_OCR_{method}.bin\"))\n    print(f'Saved {os.path.join(bin_path, f\"faiss_OCR_{method}.bin\")}')\n\n\n# write ocr\nwrite_bin_file_ocr(bin_path=f\"{WORK_DIR}/data/dicts/bin_ocr\", npy_path=f\"{WORK_DIR}/data/dicts/npy_ocr\")\n\n","metadata":{},"execution_count":null,"outputs":[]}]}