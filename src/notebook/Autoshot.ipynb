{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1wMPZLyEHtH_lhac1U0NOkba2nlg01o0o","authorship_tag":"ABX9TyP25eKlYiVLINSvkQWcbAw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gogy64a7R22K","executionInfo":{"status":"ok","timestamp":1722779638057,"user_tz":-420,"elapsed":9128,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"78f6cde7-99a9-4840-ed49-0a10a25bb3a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'AutoShot'...\n","remote: Enumerating objects: 66, done.\u001b[K\n","remote: Counting objects: 100% (66/66), done.\u001b[K\n","remote: Compressing objects: 100% (66/66), done.\u001b[K\n","remote: Total 66 (delta 31), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (66/66), 18.82 MiB | 17.49 MiB/s, done.\n","Resolving deltas: 100% (31/31), done.\n","fatal: destination path 'TransNetV2' already exists and is not an empty directory.\n","Collecting einops==0.4.1\n","  Downloading einops-0.4.1-py3-none-any.whl.metadata (10 kB)\n","Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["!git clone https://github.com/wentaozhu/AutoShot.git\n","\n","!git clone https://github.com/soCzech/TransNetV2.git\n"]},{"cell_type":"code","source":["!pip3 install --force einops==0.4.1\n","!pip install -q ffmpeg-python pillow"],"metadata":{"id":"vmn_Bov0q2k8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722788312936,"user_tz":-420,"elapsed":19684,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"81fc5d8c-f2c2-45e6-c281-47c19495001c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops==0.4.1\n","  Downloading einops-0.4.1-py3-none-any.whl.metadata (10 kB)\n","Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}]},{"cell_type":"code","source":["!python3 /content/drive/MyDrive/AIC24/model/AutoShot/compare_inference_baseline_groundtruth_v2.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L0byTn9zSYZ8","executionInfo":{"status":"ok","timestamp":1722788329060,"user_tz":-420,"elapsed":16133,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"f2591d4f-d721-4ba0-c9af-923422a0db7d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pretrained weights from /content/drive/MyDrive/AIC24/model/ckpt_0_200_0.pth\n","Current model has 90 parameters, updated parameters 90\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/AIC24/model/AutoShot/compare_inference_baseline_groundtruth_v2.py\", line 54, in <module>\n","    for fnm in os.listdir(video_dir):\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/video_download'\n"]}]},{"cell_type":"code","source":["import shutil\n","shutil.make_archive('/content/outputframes', 'zip', '/content/predicted_frames')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wqcjPYMn0RlC","executionInfo":{"status":"ok","timestamp":1722782241768,"user_tz":-420,"elapsed":379,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"c6744f1e-5f68-44cb-8f9e-fcba1413afd2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/outputframes.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import os\n","\n","_, _, files = next(os.walk(\"/content/predicted_frames\"))\n","file_count = len(files)\n","print (file_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoMTYlMw0q4j","executionInfo":{"status":"ok","timestamp":1722782725616,"user_tz":-420,"elapsed":379,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"beceed6f-acc1-4fad-d7fb-094ba15b8aec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["505\n"]}]},{"cell_type":"code","source":["import cv2\n","import os\n","import torch\n","from transnet_v2 import TransNetV2  # Import the TransNetV2 model class from its module\n","\n","# Define the directory to save frames\n","output_dir = 'scene_frames'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Function to save frames from each scene\n","def save_frames_from_scenes(video_path, scenes, num_frames_per_scene=3):\n","    # Open the video file\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(f\"Error opening video file: {video_path}\")\n","        return\n","\n","    # Get video properties\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    # Iterate through each scene\n","    for scene_index, (start_frame, end_frame) in enumerate(scenes):\n","        # Ensure the frame indices are within the video length\n","        start_frame = max(0, start_frame)\n","        end_frame = min(total_frames - 1, end_frame)\n","\n","        # Compute frames to extract from the current scene\n","        scene_frames = list(range(start_frame, end_frame + 1))\n","        step = max(len(scene_frames) // num_frames_per_scene, 1)\n","        sampled_frames = scene_frames[::step][:num_frames_per_scene]\n","\n","        # Save frames for this scene\n","        for idx, frame_number in enumerate(sampled_frames):\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n","            ret, frame = cap.read()\n","            if ret:\n","                frame_filename = os.path.join(output_dir, f\"scene_{scene_index}_frame_{idx}.jpg\")\n","                cv2.imwrite(frame_filename, frame)\n","                print(f\"Saved frame {frame_number} from scene {scene_index} as {frame_filename}\")\n","            else:\n","                print(f\"Failed to read frame {frame_number} from video\")\n","\n","    # Release the video capture object\n","    cap.release()\n","\n","# Load the TransNetV2 model\n","def load_transnet_v2_model(model_path):\n","    # Initialize the model\n","    model = TransNetV2()\n","\n","    # Load pre-trained weights\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()  # Set the model to evaluation mode\n","\n","    return model\n","\n","# Example usage\n","video_path = \"L01_V001.mp4\"\n","model_path = \"path_to_transnetv2_weights.pth\"  # Path to your model weights file\n","\n","# Load the model\n","model = load_transnet_v2_model(model_path)\n","\n","# Predict scenes in the video\n","_, single_frame_predictions, _ = model.predict_video(video_path)\n","scenes = model.predictions_to_scenes(single_frame_predictions)\n","\n","# Save 3 frames from each scene\n","save_frames_from_scenes(video_path, scenes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"2y6FtIpY9OnA","executionInfo":{"status":"error","timestamp":1722784494337,"user_tz":-420,"elapsed":371,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}},"outputId":"c9a56e28-650c-46eb-f4c8-8af1b56a2254"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'TransNetV2' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1c39ead32db4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'TransNetV2' is not defined"]}]}]}