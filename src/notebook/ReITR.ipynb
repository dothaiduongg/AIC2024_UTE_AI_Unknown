{"cells":[{"cell_type":"markdown","metadata":{"id":"oB86HK9a7nyI"},"source":["## <strong>RelTR Online Demo</strong>\n","\n","This is a online notebook that shows how RelTR model detects a fixed-length set of relationships given an image. The loaded checkpoint is pretrained on Visual Genome-150."]},{"cell_type":"markdown","metadata":{"id":"McZM9W-idDUq"},"source":["# Installation\n","Install RelTR and import necessary packages."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25110,"status":"ok","timestamp":1722785602224,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"m2TH79Ufb5Fp","outputId":"f1992e06-388a-415e-a293-b1110ddbc7a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RelTR'...\n","remote: Enumerating objects: 317, done.\u001b[K\n","remote: Counting objects: 100% (317/317), done.\u001b[K\n","remote: Compressing objects: 100% (175/175), done.\u001b[K\n","remote: Total 317 (delta 140), reused 282 (delta 120), pack-reused 0\u001b[K\n","Receiving objects: 100% (317/317), 27.49 MiB | 26.43 MiB/s, done.\n","Resolving deltas: 100% (140/140), done.\n","/content/RelTR\n"]}],"source":["!git clone https://github.com/yrcong/RelTR.git\n","%cd RelTR/\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","\n","from PIL import Image\n","import requests\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"VmHBU0_LdqWH"},"source":["# VG labels\n","VG 150 enitiy classes and 50 relationship classes."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aZfsXdDzdiPG","executionInfo":{"status":"ok","timestamp":1722785602227,"user_tz":-420,"elapsed":7,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}}},"outputs":[],"source":["CLASSES = [ 'N/A', 'airplane', 'animal', 'arm', 'bag', 'banana', 'basket', 'beach', 'bear', 'bed', 'bench', 'bike',\n","                'bird', 'board', 'boat', 'book', 'boot', 'bottle', 'bowl', 'box', 'boy', 'branch', 'building',\n","                'bus', 'cabinet', 'cap', 'car', 'cat', 'chair', 'child', 'clock', 'coat', 'counter', 'cow', 'cup',\n","                'curtain', 'desk', 'dog', 'door', 'drawer', 'ear', 'elephant', 'engine', 'eye', 'face', 'fence',\n","                'finger', 'flag', 'flower', 'food', 'fork', 'fruit', 'giraffe', 'girl', 'glass', 'glove', 'guy',\n","                'hair', 'hand', 'handle', 'hat', 'head', 'helmet', 'hill', 'horse', 'house', 'jacket', 'jean',\n","                'kid', 'kite', 'lady', 'lamp', 'laptop', 'leaf', 'leg', 'letter', 'light', 'logo', 'man', 'men',\n","                'motorcycle', 'mountain', 'mouth', 'neck', 'nose', 'number', 'orange', 'pant', 'paper', 'paw',\n","                'people', 'person', 'phone', 'pillow', 'pizza', 'plane', 'plant', 'plate', 'player', 'pole', 'post',\n","                'pot', 'racket', 'railing', 'rock', 'roof', 'room', 'screen', 'seat', 'sheep', 'shelf', 'shirt',\n","                'shoe', 'short', 'sidewalk', 'sign', 'sink', 'skateboard', 'ski', 'skier', 'sneaker', 'snow',\n","                'sock', 'stand', 'street', 'surfboard', 'table', 'tail', 'tie', 'tile', 'tire', 'toilet', 'towel',\n","                'tower', 'track', 'train', 'tree', 'truck', 'trunk', 'umbrella', 'vase', 'vegetable', 'vehicle',\n","                'wave', 'wheel', 'window', 'windshield', 'wing', 'wire', 'woman', 'zebra']\n","\n","REL_CLASSES = ['__background__', 'above', 'across', 'against', 'along', 'and', 'at', 'attached to', 'behind',\n","                'belonging to', 'between', 'carrying', 'covered in', 'covering', 'eating', 'flying in', 'for',\n","                'from', 'growing on', 'hanging from', 'has', 'holding', 'in', 'in front of', 'laying on',\n","                'looking at', 'lying on', 'made of', 'mounted on', 'near', 'of', 'on', 'on back of', 'over',\n","                'painted on', 'parked on', 'part of', 'playing', 'riding', 'says', 'sitting on', 'standing on',\n","                'to', 'under', 'using', 'walking in', 'walking on', 'watching', 'wearing', 'wears', 'with']\n"]},{"cell_type":"markdown","metadata":{"id":"qNb3xdkZeKy6"},"source":["# Build and load the pretrained model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"executionInfo":{"elapsed":13313,"status":"error","timestamp":1722785615534,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"TeWdzd5LeOGQ","outputId":"971237dd-9ab2-4c1c-decc-77f1d67547be"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:04<00:00, 25.0MB/s]\n","Downloading: \"https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth\" to /root/.cache/torch/hub/checkpoints/checkpoint0149.pth\n"]},{"output_type":"error","ename":"HTTPError","evalue":"HTTP Error 404: Not Found","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6d3285766ff9>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# The checkpoint is pretrained on Visual Genome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m ckpt = torch.hub.load_state_dict_from_url(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     map_location='cpu', check_hash=True)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# r is Optional[Match[str]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0mfile_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"torch.hub\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getheaders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"]}],"source":["from models.backbone import Backbone, Joiner\n","from models.position_encoding import PositionEmbeddingSine\n","from models.transformer import Transformer\n","from models.reltr import RelTR\n","\n","position_embedding = PositionEmbeddingSine(128, normalize=True)\n","backbone = Backbone('resnet50', False, False, False)\n","backbone = Joiner(backbone, position_embedding)\n","backbone.num_channels = 2048\n","\n","transformer = Transformer(d_model=256, dropout=0.1, nhead=8,\n","                          dim_feedforward=2048,\n","                          num_encoder_layers=6,\n","                          num_decoder_layers=6,\n","                          normalize_before=False,\n","                          return_intermediate_dec=True)\n","\n","model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n","              num_entities=100, num_triplets=200)\n","\n","# The checkpoint is pretrained on Visual Genome\n","ckpt = torch.hub.load_state_dict_from_url(\n","    url='https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth',\n","    map_location='cpu', check_hash=True)\n","model.load_state_dict(ckpt['model'])\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58v9rajd1ONO","executionInfo":{"status":"aborted","timestamp":1722785615535,"user_tz":-420,"elapsed":7,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}}},"outputs":[],"source":["# Some transformation functions\n","transform = T.Compose([\n","    T.Resize(800),\n","    T.ToTensor(),\n","    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","# for output bounding box post-processing\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n","          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","def rescale_bboxes(out_bbox, size):\n","    img_w, img_h = size\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","    return b"]},{"cell_type":"markdown","metadata":{"id":"4nGtLQrmAfJB"},"source":["# Load Image\n","You can replace the link with other images. Note that the entities in the used image should be included in the VG labels."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1722785615535,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"010vjjIsFjCF"},"outputs":[],"source":["url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Yellow_Happy.jpg/1200px-Yellow_Happy.jpg'\n","im = Image.open(requests.get(url, stream=True).raw)\n","plt.imshow(im)\n","img = transform(im).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"Rx2Wez2qBdsE"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdDlFttuF-7k","executionInfo":{"status":"aborted","timestamp":1722785615536,"user_tz":-420,"elapsed":8,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}}},"outputs":[],"source":["# propagate through the model\n","outputs = model(img)\n","\n","# keep only predictions with >0.3 confidence\n","probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n","probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n","probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n","keep = torch.logical_and(probas.max(-1).values > 0.3, torch.logical_and(probas_sub.max(-1).values > 0.3,\n","                                                                        probas_obj.max(-1).values > 0.3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZKzADKFGN_e","executionInfo":{"status":"aborted","timestamp":1722785615536,"user_tz":-420,"elapsed":7,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}}},"outputs":[],"source":["# convert boxes from [0; 1] to image scales\n","sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n","obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n","\n","topk = 10 # display up to 10 images\n","keep_queries = torch.nonzero(keep, as_tuple=True)[0]\n","indices = torch.argsort(-probas[keep_queries].max(-1)[0] * probas_sub[keep_queries].max(-1)[0] * probas_obj[keep_queries].max(-1)[0])[:topk]\n","keep_queries = keep_queries[indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGjLhwwfGW7U","executionInfo":{"status":"aborted","timestamp":1722785615536,"user_tz":-420,"elapsed":7,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"}}},"outputs":[],"source":["# save the attention weights\n","conv_features, dec_attn_weights_sub, dec_attn_weights_obj = [], [], []\n","hooks = [\n","    model.backbone[-2].register_forward_hook(\n","        lambda self, input, output: conv_features.append(output)\n","    ),\n","    model.transformer.decoder.layers[-1].cross_attn_sub.register_forward_hook(\n","        lambda self, input, output: dec_attn_weights_sub.append(output[1])\n","    ),\n","    model.transformer.decoder.layers[-1].cross_attn_obj.register_forward_hook(\n","        lambda self, input, output: dec_attn_weights_obj.append(output[1])\n","    )]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":380,"status":"error","timestamp":1722785622823,"user":{"displayName":"Triệu Huy Phan","userId":"10945881926721216773"},"user_tz":-420},"id":"N5O9HJuFGc4W","outputId":"0fdde114-729e-480e-ed42-0a994ec83aa4"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'img' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-50bc85d7936c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# propagate through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"]}],"source":["with torch.no_grad():\n","    # propagate through the model\n","    outputs = model(img)\n","\n","    for hook in hooks:\n","        hook.remove()\n","\n","    # don't need the list anymore\n","    conv_features = conv_features[0]\n","    dec_attn_weights_sub = dec_attn_weights_sub[0]\n","    dec_attn_weights_obj = dec_attn_weights_obj[0]\n","\n","    # get the feature map shape\n","    h, w = conv_features['0'].tensors.shape[-2:]\n","    im_w, im_h = im.size\n","\n","    fig, axs = plt.subplots(ncols=len(indices), nrows=3, figsize=(22, 7))\n","    for idx, ax_i, (sxmin, symin, sxmax, symax), (oxmin, oymin, oxmax, oymax) in \\\n","            zip(keep_queries, axs.T, sub_bboxes_scaled[indices], obj_bboxes_scaled[indices]):\n","        ax = ax_i[0]\n","        ax.imshow(dec_attn_weights_sub[0, idx].view(h, w))\n","        ax.axis('off')\n","        ax.set_title(f'query id: {idx.item()}')\n","        ax = ax_i[1]\n","        ax.imshow(dec_attn_weights_obj[0, idx].view(h, w))\n","        ax.axis('off')\n","        ax = ax_i[2]\n","        ax.imshow(im)\n","        ax.add_patch(plt.Rectangle((sxmin, symin), sxmax - sxmin, symax - symin,\n","                                    fill=False, color='blue', linewidth=2.5))\n","        ax.add_patch(plt.Rectangle((oxmin, oymin), oxmax - oxmin, oymax - oymin,\n","                                    fill=False, color='orange', linewidth=2.5))\n","\n","        ax.axis('off')\n","        ax.set_title(CLASSES[probas_sub[idx].argmax()]+' '+REL_CLASSES[probas[idx].argmax()]+' '+CLASSES[probas_obj[idx].argmax()], fontsize=10)\n","        print( CLASSES[probas_sub[idx].argmax()]+' '+REL_CLASSES[probas[idx].argmax()]+' '+CLASSES[probas_obj[idx].argmax()], fontsize=10)\n","    fig.tight_layout()\n","    plt.show() # show the output\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1l8LXOOlp4x2vMDneiBN_3DhMWaB3MRXG","timestamp":1722785446571},{"file_id":"1-U642OoCyb8OSM8nx9lme49dmWa_aUcU","timestamp":1722785366171}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}